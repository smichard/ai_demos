{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import warnings\n",
    "from urllib3.exceptions import InsecureRequestWarning\n",
    "\n",
    "# Hide warnings about insecure TLS connections\n",
    "warnings.filterwarnings(\"ignore\", category=InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "API_ENDPOINT = \"<inference_endpoint>/v1/chat/completions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def chat_completion(messages, \n",
    "                    model=\"google/gemma-2-2b-instruct\",\n",
    "                    top_p=1,\n",
    "                    n=1,\n",
    "                    max_tokens=50,\n",
    "                    stream=False,\n",
    "                    frequency_penalty=0.0,\n",
    "                    stop=None):\n",
    "    \"\"\"\n",
    "    Calls the /v1/chat/completions endpoint on the NIM container.\n",
    "    Expects a list of messages for a 'chat' style model.\n",
    "    \"\"\"\n",
    "    # 2. Build the payload according to the logs\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"top_p\": top_p,\n",
    "        \"n\": n,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"stream\": stream,\n",
    "        \"frequency_penalty\": frequency_penalty\n",
    "    }\n",
    "    \n",
    "    # Optional: if you want to specify stop tokens\n",
    "    if stop is not None:\n",
    "        payload[\"stop\"] = stop\n",
    "\n",
    "    try:\n",
    "        # 3. Make the POST request\n",
    "        response = requests.post(\n",
    "            API_ENDPOINT,\n",
    "            headers={\"Content-Type\": \"application/json\"},\n",
    "            data=json.dumps(payload),\n",
    "            timeout=30,\n",
    "            verify=False\n",
    "        )\n",
    "        # 4. Raise exception on non-2xx response\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        print(f\"Request error: {err}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 5. Example usage: a small chat\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": \"Hello! How are you?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Hi! I'm well, how can I help you today?\"},\n",
    "        {\"role\": \"user\", \"content\": \"Can you write me a short poem?\"}\n",
    "    ]\n",
    "    \n",
    "    result = chat_completion(\n",
    "        messages,\n",
    "        max_tokens=30,        # number of tokens in the response\n",
    "        stream=False,         # if True, you'd have to handle chunks\n",
    "        frequency_penalty=1.0 # as in your example\n",
    "    )\n",
    "    \n",
    "    print(\"Inference result:\", result)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
